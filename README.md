# Deep Uncertainty

# Packages
 ## Deep Ensembles

This package reproduces the results found in the paper *Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles*. [![arXiv](http://img.shields.io/badge/arXiv-1612.01474-B31B1B.svg)](https://arxiv.org/abs/1612.01474)

This package is the result of code taken/adapted from different GitHub repositories:  
    - [Deep Ensemble, mpritzkoleit](https://github.com/mpritzkoleit/deep-ensembles/blob/master/Deep%20Ensembles.ipynb)

## Loss Landscapes

This package follows the [Fort, S., Hu, H., & Lakshminarayanan, B. (2019). Deep ensembles: A loss landscape perspective. arXiv preprint arXiv:1912.02757.](https://arxiv.org/abs/1912.02757).

## SGD Selection

Important properties of SGD optimization and it's escape property are analysed here, following: [Wu, L., & Ma, C. (2018). How sgd selects the global minima in over-parameterized learning: A dynamical stability perspective. Advances in Neural Information Processing Systems, 31.](https://proceedings.neurips.cc/paper/2018/hash/6651526b6fb8f29a00507de6a49ce30f-Abstract.html)


# Datasets

- [Boston Housing](https://www.kaggle.com/datasets/vikrishnan/boston-house-prices?resource=download)
- [Concrete Strength](https://www.kaggle.com/datasets/elikplim/concrete-compressive-strength-data-set)
- [Energy Efficiency](https://www.kaggle.com/datasets/elikplim/eergy-efficiency-dataset)
- [Kin8nm](https://www.openml.org/search?type=data&sort=runs&id=189&status=active)
- [Naval Propulsion Plant](https://www.kaggle.com/datasets/elikplim/maintenance-of-naval-propulsion-plants-data-set?select=navalplantmaintenance.csv)
- [Power Plant](https://www.kaggle.com/datasets/eshaan90/global-power-plant-database?select=global_power_plant_database.csv)
- [Protein Structure](https://www.kaggle.com/datasets/shahir/protein-data-set)
- [Wine Quality Red](https://www.kaggle.com/datasets/uciml/red-wine-quality-cortez-et-al-2009)
- [Yacht Hydrodynamics](https://www.kaggle.com/datasets/heitornunes/yacht-hydrodynamics-data-set)
- [Year Prediction MSD](https://archive.ics.uci.edu/ml/machine-learning-databases/00203/)
- [MNIST](http://yann.lecun.com/exdb/mnist/)
- [SVHN](http://ufldl.stanford.edu/housenumbers/)
  
# References
- [Fort, S., Hu, H., & Lakshminarayanan, B. (2019). Deep ensembles: A loss landscape perspective. arXiv preprint arXiv:1912.02757.](https://arxiv.org/abs/1912.02757)
- [Gal, Y., & Ghahramani, Z. (2016, June). Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In international conference on machine learning (pp. 1050-1059). PMLR.](http://proceedings.mlr.press/v48/gal16.html?ref=https://githubhelp.com)
- [Hern√°ndez-Lobato, J. M., & Adams, R. (2015, June). Probabilistic backpropagation for scalable learning of bayesian neural networks. In International conference on machine learning (pp. 1861-1869). PMLR.](http://proceedings.mlr.press/v37/hernandez-lobatoc15.html).
- [Lakshminarayanan, B., Pritzel, A., & Blundell, C. (2017). Simple and scalable predictive uncertainty estimation using deep ensembles. Advances in neural information processing systems, 30.](https://proceedings.neurips.cc/paper/2017/hash/9ef2ed4b7fd2c810847ffa5fa85bce38-Abstract.html).
- [Wu, L., & Ma, C. (2018). How sgd selects the global minima in over-parameterized learning: A dynamical stability perspective. Advances in Neural Information Processing Systems, 31.](https://proceedings.neurips.cc/paper/2018/hash/6651526b6fb8f29a00507de6a49ce30f-Abstract.html)
